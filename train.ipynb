{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHzTmPwQQ3krneMff7RSy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richmondvan/melanoma-detection/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GBnQEFeOgSK",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZUfw13gOyhU",
        "colab_type": "text"
      },
      "source": [
        "Import all modules and mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kAzzn_kNT3v",
        "colab_type": "code",
        "outputId": "6f6a920d-b4a0-4ece-df34-598ef71a3a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Must be run every time!\n",
        "\n",
        "import pathlib # Manage file paths\n",
        "import math # Manage basic math\n",
        "import pickle # Storing epoch number\n",
        "import csv # Storing data in .csv files\n",
        "from google.colab import drive # For mounting GDrive\n",
        "import tensorflow_hub as hub # For importing EfficientNet\n",
        "\n",
        "import tensorflow as tf #nightly\n",
        "from tensorflow.keras import models, layers, losses, metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzfDku_bO18a",
        "colab_type": "text"
      },
      "source": [
        "Prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqBjcCuNjMG",
        "colab_type": "code",
        "outputId": "78994329-aa95-4f3e-fbb4-9c3d8fb188bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Setting up file paths\n",
        "PATH = \"/content/gdrive/My Drive/Dataset/DatasetSorted/\"\n",
        "\n",
        "TRAINING_PATH = pathlib.Path(PATH + \"training/\")\n",
        "VALIDATION_PATH = pathlib.Path(PATH + \"validation/\")\n",
        "TEST_PATH = pathlib.Path(PATH + \"test/\")\n",
        "\n",
        "# Create image generators\n",
        "train_image_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    brightness_range=(0.95, 1.05),\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True) # Generator for our validation data\n",
        "\n",
        "# Some constants\n",
        "batch_size = 32\n",
        "IMG_HEIGHT = 600\n",
        "IMG_WIDTH = 600\n",
        "TRAIN_LEN = len(list(TRAINING_PATH.glob(\"*/*.jpg\")))\n",
        "VALID_LEN = len(list(VALIDATION_PATH.glob(\"*/*.jpg\")))\n",
        "CLASS_NAMES = ['benign', 'malignant']\n",
        "\n",
        "# Get generated datasets\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=TRAINING_PATH,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary',\n",
        "                                                           classes=CLASS_NAMES)\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=VALIDATION_PATH,\n",
        "                                                              shuffle=True,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary',\n",
        "                                                              classes=CLASS_NAMES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8423 images belonging to 2 classes.\n",
            "Found 1054 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U--YerHEO7gD",
        "colab_type": "text"
      },
      "source": [
        "Prepare metrics and weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9iRPtTtNmHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get some training weights to offset class imbalance\n",
        "numBenign = len(list(TRAINING_PATH.glob(\"benign/*.jpg\")))\n",
        "numMalignant = len(list(TRAINING_PATH.glob(\"malignant/*.jpg\")))\n",
        "total = numBenign + numMalignant\n",
        "weight_for_0 = (1 / numBenign)*(total)/2.0 \n",
        "weight_for_1 = (1.5 / numMalignant)*(total)/2.0\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "# Metrics we will be using to assess accuracy\n",
        "METRICS = [\n",
        "      metrics.TruePositives(name='tp'),\n",
        "      metrics.FalsePositives(name='fp'),\n",
        "      metrics.TrueNegatives(name='tn'),\n",
        "      metrics.FalseNegatives(name='fn'), \n",
        "      metrics.BinaryAccuracy(name='accuracy'),\n",
        "      metrics.Precision(name='precision'),\n",
        "      metrics.Recall(name='recall'),\n",
        "      metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmZ-5AzPZxU",
        "colab_type": "text"
      },
      "source": [
        "Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFVGzhJeNrat",
        "colab_type": "code",
        "outputId": "a403fbd9-0f0f-4cea-f0e2-63179e69d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# Hyperparameters\n",
        "NEURONS_PER_LAYER = 1024\n",
        "REG_LAMBDA = 0.001\n",
        "DROPOUT = 0.1\n",
        "ACTIVATION = \"relu\"\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/efficientnet/b7/feature-vector/1\", trainable=False),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=METRICS)\n",
        "\n",
        "model.build([None, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     multiple                  64097680  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  2622464   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  1049600   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  1049600   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  1049600   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  1025      \n",
            "=================================================================\n",
            "Total params: 69,869,969\n",
            "Trainable params: 5,772,289\n",
            "Non-trainable params: 64,097,680\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4lfi_UpPcDj",
        "colab_type": "text"
      },
      "source": [
        "Load model weights and last epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVuJBRAEGbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3e965d83-fad5-438c-96ce-d191f661fd99"
      },
      "source": [
        "# Get last epoch number from pickled file\n",
        "\n",
        "EPOCH_FILEPATH = f\"/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_4_epochnum\"\n",
        "\n",
        "infile = open(EPOCH_FILEPATH, 'rb')\n",
        "try: \n",
        "    infile.seek(0)\n",
        "    epoch = pickle.load(infile)\n",
        "    model.load_weights(f\"/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_4/epoch{epoch}.h5\")\n",
        "except: \n",
        "    # Otherwise start again (only happens if no epoch number found)\n",
        "    epoch = 0\n",
        "\n",
        "\n",
        "print(epoch)\n",
        "infile.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Dataset/enet1024_4_epochnum\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIC8UGzFk7AJ",
        "colab_type": "text"
      },
      "source": [
        "Prepare CSV logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLyBNCPteyrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File where we store our CSV history\n",
        "\n",
        "HISTORY_FILE = f'/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_stats.csv'\n",
        "\n",
        "csv_logger = CSVLogger(HISTORY_FILE, append=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftn4Tb_LM2d1",
        "colab_type": "text"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1bTVD9HNsvj",
        "colab_type": "code",
        "outputId": "3410acd5-9ad8-4de3-c1b4-aa9eb5cb9c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Train for 150 epochs\n",
        "\n",
        "if epoch < 100:\n",
        "    for i in range(epoch, 100):\n",
        "        history = model.fit(x=train_data_gen, \n",
        "                            epochs=i+1, \n",
        "                            initial_epoch=i, \n",
        "                            verbose=1, \n",
        "                            validation_data=val_data_gen, \n",
        "                            validation_steps=VALID_LEN // batch_size, \n",
        "                            steps_per_epoch=TRAIN_LEN // batch_size, \n",
        "                            class_weight=class_weight,\n",
        "                            callbacks = [csv_logger])\n",
        "        model.save_weights(f\"/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_4/epoch{i + 1}.h5\")\n",
        "        outfile = open(EPOCH_FILEPATH, 'wb')\n",
        "        pickle.dump(i+1, outfile)\n",
        "        outfile.close()\n",
        "\n",
        "    \n",
        "    epoch = 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/17\n",
            "263/263 [==============================] - 3480s 13s/step - loss: 0.7411 - tp: 1000.0000 - fp: 1597.0000 - tn: 5489.0000 - fn: 330.0000 - accuracy: 0.7710 - precision: 0.3851 - recall: 0.7519 - auc: 0.8239 - val_loss: 0.7887 - val_tp: 137.0000 - val_fp: 209.0000 - val_tn: 649.0000 - val_fn: 29.0000 - val_accuracy: 0.7676 - val_precision: 0.3960 - val_recall: 0.8253 - val_auc: 0.8553\n",
            "Epoch 18/18\n",
            "263/263 [==============================] - 906s 3s/step - loss: 0.7371 - tp: 1022.0000 - fp: 1582.0000 - tn: 5502.0000 - fn: 310.0000 - accuracy: 0.7752 - precision: 0.3925 - recall: 0.7673 - auc: 0.8216 - val_loss: 0.7436 - val_tp: 109.0000 - val_fp: 120.0000 - val_tn: 739.0000 - val_fn: 56.0000 - val_accuracy: 0.8281 - val_precision: 0.4760 - val_recall: 0.6606 - val_auc: 0.8447\n",
            "Epoch 19/19\n",
            "263/263 [==============================] - 902s 3s/step - loss: 0.7406 - tp: 1020.0000 - fp: 1687.0000 - tn: 5397.0000 - fn: 312.0000 - accuracy: 0.7625 - precision: 0.3768 - recall: 0.7658 - auc: 0.8233 - val_loss: 0.7840 - val_tp: 132.0000 - val_fp: 198.0000 - val_tn: 663.0000 - val_fn: 31.0000 - val_accuracy: 0.7764 - val_precision: 0.4000 - val_recall: 0.8098 - val_auc: 0.8472\n",
            "Epoch 20/20\n",
            "263/263 [==============================] - 904s 3s/step - loss: 0.7417 - tp: 1058.0000 - fp: 1897.0000 - tn: 5188.0000 - fn: 273.0000 - accuracy: 0.7422 - precision: 0.3580 - recall: 0.7949 - auc: 0.8131 - val_loss: 0.8042 - val_tp: 133.0000 - val_fp: 234.0000 - val_tn: 630.0000 - val_fn: 27.0000 - val_accuracy: 0.7451 - val_precision: 0.3624 - val_recall: 0.8313 - val_auc: 0.8489\n",
            "Epoch 21/21\n",
            "263/263 [==============================] - 896s 3s/step - loss: 0.7489 - tp: 1023.0000 - fp: 1842.0000 - tn: 5242.0000 - fn: 309.0000 - accuracy: 0.7444 - precision: 0.3571 - recall: 0.7680 - auc: 0.7941 - val_loss: 0.7584 - val_tp: 120.0000 - val_fp: 145.0000 - val_tn: 720.0000 - val_fn: 39.0000 - val_accuracy: 0.8203 - val_precision: 0.4528 - val_recall: 0.7547 - val_auc: 0.8420\n",
            "Epoch 22/22\n",
            "263/263 [==============================] - 899s 3s/step - loss: 0.7380 - tp: 1036.0000 - fp: 1712.0000 - tn: 5372.0000 - fn: 296.0000 - accuracy: 0.7614 - precision: 0.3770 - recall: 0.7778 - auc: 0.8299 - val_loss: 0.7580 - val_tp: 121.0000 - val_fp: 152.0000 - val_tn: 709.0000 - val_fn: 42.0000 - val_accuracy: 0.8105 - val_precision: 0.4432 - val_recall: 0.7423 - val_auc: 0.8551\n",
            "Epoch 23/23\n",
            "263/263 [==============================] - 903s 3s/step - loss: 0.7412 - tp: 1006.0000 - fp: 1624.0000 - tn: 5461.0000 - fn: 325.0000 - accuracy: 0.7684 - precision: 0.3825 - recall: 0.7558 - auc: 0.8113 - val_loss: 0.8336 - val_tp: 140.0000 - val_fp: 286.0000 - val_tn: 579.0000 - val_fn: 19.0000 - val_accuracy: 0.7021 - val_precision: 0.3286 - val_recall: 0.8805 - val_auc: 0.8212\n",
            "Epoch 24/24\n",
            "263/263 [==============================] - 904s 3s/step - loss: 0.7357 - tp: 1037.0000 - fp: 1654.0000 - tn: 5430.0000 - fn: 295.0000 - accuracy: 0.7684 - precision: 0.3854 - recall: 0.7785 - auc: 0.8248 - val_loss: 0.7197 - val_tp: 79.0000 - val_fp: 63.0000 - val_tn: 796.0000 - val_fn: 86.0000 - val_accuracy: 0.8545 - val_precision: 0.5563 - val_recall: 0.4788 - val_auc: 0.8031\n",
            "Epoch 25/25\n",
            "263/263 [==============================] - 903s 3s/step - loss: 0.7344 - tp: 1032.0000 - fp: 1547.0000 - tn: 5538.0000 - fn: 299.0000 - accuracy: 0.7807 - precision: 0.4002 - recall: 0.7754 - auc: 0.8110 - val_loss: 0.7740 - val_tp: 125.0000 - val_fp: 175.0000 - val_tn: 687.0000 - val_fn: 37.0000 - val_accuracy: 0.7930 - val_precision: 0.4167 - val_recall: 0.7716 - val_auc: 0.8301\n",
            "Epoch 26/26\n",
            "263/263 [==============================] - 898s 3s/step - loss: 0.7389 - tp: 1042.0000 - fp: 1754.0000 - tn: 5330.0000 - fn: 290.0000 - accuracy: 0.7571 - precision: 0.3727 - recall: 0.7823 - auc: 0.8205 - val_loss: 0.7548 - val_tp: 124.0000 - val_fp: 145.0000 - val_tn: 718.0000 - val_fn: 37.0000 - val_accuracy: 0.8223 - val_precision: 0.4610 - val_recall: 0.7702 - val_auc: 0.8557\n",
            "Epoch 27/27\n",
            "263/263 [==============================] - 918s 3s/step - loss: 0.7377 - tp: 1013.0000 - fp: 1596.0000 - tn: 5491.0000 - fn: 316.0000 - accuracy: 0.7728 - precision: 0.3883 - recall: 0.7622 - auc: 0.8212 - val_loss: 0.7459 - val_tp: 112.0000 - val_fp: 124.0000 - val_tn: 737.0000 - val_fn: 51.0000 - val_accuracy: 0.8291 - val_precision: 0.4746 - val_recall: 0.6871 - val_auc: 0.8410\n",
            "Epoch 28/28\n",
            "263/263 [==============================] - 921s 4s/step - loss: 0.7329 - tp: 1084.0000 - fp: 1812.0000 - tn: 5272.0000 - fn: 248.0000 - accuracy: 0.7552 - precision: 0.3743 - recall: 0.8138 - auc: 0.8205 - val_loss: 0.8562 - val_tp: 149.0000 - val_fp: 330.0000 - val_tn: 527.0000 - val_fn: 18.0000 - val_accuracy: 0.6602 - val_precision: 0.3111 - val_recall: 0.8922 - val_auc: 0.8068\n",
            "Epoch 29/29\n",
            "263/263 [==============================] - 924s 4s/step - loss: 0.7385 - tp: 1066.0000 - fp: 1859.0000 - tn: 5226.0000 - fn: 265.0000 - accuracy: 0.7476 - precision: 0.3644 - recall: 0.8009 - auc: 0.8157 - val_loss: 0.7843 - val_tp: 124.0000 - val_fp: 200.0000 - val_tn: 662.0000 - val_fn: 38.0000 - val_accuracy: 0.7676 - val_precision: 0.3827 - val_recall: 0.7654 - val_auc: 0.8375\n",
            "Epoch 30/30\n",
            "263/263 [==============================] - 929s 4s/step - loss: 0.7383 - tp: 1015.0000 - fp: 1603.0000 - tn: 5482.0000 - fn: 316.0000 - accuracy: 0.7720 - precision: 0.3877 - recall: 0.7626 - auc: 0.8152 - val_loss: 0.7439 - val_tp: 111.0000 - val_fp: 123.0000 - val_tn: 739.0000 - val_fn: 51.0000 - val_accuracy: 0.8301 - val_precision: 0.4744 - val_recall: 0.6852 - val_auc: 0.8162\n",
            "Epoch 31/31\n",
            "263/263 [==============================] - 923s 4s/step - loss: 0.7376 - tp: 1015.0000 - fp: 1587.0000 - tn: 5497.0000 - fn: 317.0000 - accuracy: 0.7738 - precision: 0.3901 - recall: 0.7620 - auc: 0.8065 - val_loss: 0.7312 - val_tp: 100.0000 - val_fp: 92.0000 - val_tn: 768.0000 - val_fn: 64.0000 - val_accuracy: 0.8477 - val_precision: 0.5208 - val_recall: 0.6098 - val_auc: 0.8105\n",
            "Epoch 32/32\n",
            "263/263 [==============================] - 925s 4s/step - loss: 0.7427 - tp: 1039.0000 - fp: 1811.0000 - tn: 5273.0000 - fn: 293.0000 - accuracy: 0.7500 - precision: 0.3646 - recall: 0.7800 - auc: 0.8074 - val_loss: 0.8265 - val_tp: 143.0000 - val_fp: 282.0000 - val_tn: 582.0000 - val_fn: 17.0000 - val_accuracy: 0.7080 - val_precision: 0.3365 - val_recall: 0.8938 - val_auc: 0.8450\n",
            "Epoch 33/33\n",
            "105/263 [==========>...................] - ETA: 8:18 - loss: 0.7364 - tp: 435.0000 - fp: 724.0000 - tn: 2097.0000 - fn: 104.0000 - accuracy: 0.7536 - precision: 0.3753 - recall: 0.8071 - auc: 0.8275"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ1roKaSNU1L",
        "colab_type": "text"
      },
      "source": [
        "Fine-tuning with trainable EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPkaFXYEctT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model again\n",
        "\n",
        "model = models.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/efficientnet/b7/feature-vector/1\", trainable=True), # Trainable this time\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(NEURONS_PER_LAYER, kernel_regularizer=tf.keras.regularizers.l2(REG_LAMBDA), activation=ACTIVATION),\n",
        "    layers.Dropout(DROPOUT),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # smaller learning rate\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=METRICS)\n",
        "\n",
        "model.build([None, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "model.summary()\n",
        "\n",
        "model.load_weights(f\"/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_4/epoch{epoch}.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKCTvoLcNYuT",
        "colab_type": "text"
      },
      "source": [
        "Train again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "and616TGNJAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train for 50 more epochs\n",
        "\n",
        "if epoch < 150:\n",
        "    for i in range(epoch, 150):\n",
        "        history = model.fit(x=train_data_gen, \n",
        "                            epochs=i+1, \n",
        "                            initial_epoch=i, \n",
        "                            verbose=1, \n",
        "                            validation_data=val_data_gen, \n",
        "                            validation_steps=VALID_LEN // batch_size, \n",
        "                            steps_per_epoch=TRAIN_LEN // batch_size, \n",
        "                            class_weight=class_weight,\n",
        "                            callbacks = [csv_logger])\n",
        "        model.save_weights(f\"/content/gdrive/My Drive/Dataset/enet{NEURONS_PER_LAYER}_4/epoch{i + 1}.h5\")\n",
        "        outfile = open(EPOCH_FILEPATH, 'wb')\n",
        "        pickle.dump(i+1, outfile)\n",
        "        outfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}