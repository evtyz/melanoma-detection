{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ipE0MVc9i5Yy"
      ],
      "toc_visible": true,
      "mount_file_id": "1Nfd_X2Qs4_93BSqM_xdhHvJeojFs1dlz",
      "authorship_tag": "ABX9TyOm/HWAJGAff04e7hL32Qd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richmondvan/melanoma-detection/blob/master/process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipE0MVc9i5Yy",
        "colab_type": "text"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-qHkshQil-y",
        "colab_type": "text"
      },
      "source": [
        "**This cell is mandatory!**\n",
        "\n",
        "- Imports modules\n",
        "\n",
        "- Mounts Google Drive\n",
        "\n",
        "- Sets up constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkoVK_mvdHpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d84b58aa-2ea8-4305-aec2-0e5e39410ef4"
      },
      "source": [
        "# Must be run every time!\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive') \n",
        "\n",
        "DATASET_FILEPATH = \"/content/gdrive/My Drive/Dataset/dataset.zip\"\n",
        "DIRECTORY_PATH = \"/content/gdrive/My Drive/Dataset/\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWt3c6tsiy1t",
        "colab_type": "text"
      },
      "source": [
        "One-time:\n",
        "Download my data from ISIC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZV2S_bKpOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://isic-archive.com/api/v1/image/download?include=all&filter={%22operator%22:%22and%22,%22operands%22:[{%22operator%22:%22in%22,%22operands%22:[{%22identifier%22:%22meta.datasetId%22,%22type%22:%22objectid%22},[%225a2ecc5e1165975c945942a4%22,%225a2ecc5e1165975c945942a2%22,%225a2ecc5d1165975c94594292%22,%225a2ecc5d1165975c9459428e%22,%225a2ecc5d1165975c94594284%22,%225aaf6f2a116597691367292e%22,%225a2ecc5d1165975c9459427e%22,%225a2ecc5d1165975c9459428a%22]]},{%22operator%22:%22and%22,%22operands%22:[{%22operator%22:%22in%22,%22operands%22:[{%22identifier%22:%22meta.clinical.benign_malignant%22,%22type%22:%22string%22},[%22benign%22,%22malignant%22]]},{%22operator%22:%22in%22,%22operands%22:[{%22identifier%22:%22meta.tags%22,%22type%22:%22string%22},[%22Challenge%202019:%20Training%22]]}]}]}\", DATASET_FILEPATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6J7W_5jEck",
        "colab_type": "text"
      },
      "source": [
        "One-time: Extract data from archive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBe07roke8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(DATASET_FILEPATH, 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall(DIRECTORY_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnIdjEsk0iI",
        "colab_type": "text"
      },
      "source": [
        "# More Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBGQ4BA-Svk1",
        "colab_type": "text"
      },
      "source": [
        "One-time: Converts CSV file into a dictionary for classification lookups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YCPafYCHZB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "METADATA_PATH = DIRECTORY_PATH + \"ISIC-images/metadata.csv\"\n",
        "\n",
        "with open(METADATA_PATH, mode='r') as infile:\n",
        "    reader = csv.DictReader(infile)\n",
        "    GROUND_TRUTH_DICT = {row['name'] : row['meta.clinical.benign_malignant'] for row in reader} \n",
        "\n",
        "print(GROUND_TRUTH_DICT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJT5hhYfS0Fv",
        "colab_type": "text"
      },
      "source": [
        "One-time: Sort images into proper directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVqxsRGKKhWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "IMAGE_PATH = pathlib.Path(DIRECTORY_PATH + \"ISIC-images/\")\n",
        "BENIGN_PATH = \"/content/gdrive/My Drive/Dataset/DatasetSorted/benign/\"\n",
        "MALIGNANT_PATH = \"/content/gdrive/My Drive/Dataset/DatasetSorted/malignant/\"\n",
        "\n",
        "pathlist = pathlib.Path(IMAGE_PATH).glob(\"*/*.jpg\")\n",
        "for path in pathlist:\n",
        "    fileName = path.name\n",
        "    key = GROUND_TRUTH_DICT[path.name.strip(\".jpg\")]\n",
        "    if key == \"benign\":\n",
        "        pastePath = BENIGN_PATH + path.name\n",
        "    elif key == \"malignant\":\n",
        "        pastePath = MALIGNANT_PATH + path.name\n",
        "    else:\n",
        "        print(\"error\")\n",
        "    \n",
        "    copyPath = str(path)\n",
        "\n",
        "    shutil.move(copyPath, pastePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56UAJbSoTAJH",
        "colab_type": "text"
      },
      "source": [
        "**Mandatory!**\n",
        "\n",
        "Set up some functions before we load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ims7yD7BaH1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(file_path):\n",
        "    # convert the path to a list of path components\n",
        "    parts = tf.strings.split(file_path, os.path.sep)\n",
        "    # The second to last is the class-directory\n",
        "    return parts[-2] == CLASS_NAMES\n",
        "\n",
        "def decode_img(img):\n",
        "    # convert the compressed string to a 3D uint8 tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resize the image to the desired size.\n",
        "    return tf.image.resize_with_pad(img, 224, 224)\n",
        "\n",
        "def process_path(file_path):\n",
        "    label = get_label(file_path)\n",
        "    # load the raw data from the file as a string\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img)\n",
        "    return img, label\n",
        "\n",
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
        "    # This is a small dataset, only load it once, and keep it in memory.\n",
        "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "    # fit in memory.\n",
        "\n",
        "    global AUTOTUNE\n",
        "    \n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            ds = ds.cache(cache)\n",
        "        else:\n",
        "            ds = ds.cache()\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "    # Repeat forever\n",
        "    ds = ds.repeat()\n",
        "\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "    # is training.\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ6yzod5dFi5",
        "colab_type": "text"
      },
      "source": [
        "**Mandatory!**\n",
        "\n",
        "Prepare dataset for loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "926HMVJpZolG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_PATH = pathlib.Path(\"/content/gdrive/My Drive/Dataset/DatasetSorted/\")\n",
        "IMAGE_COUNT = len(list(IMAGE_PATH.glob(\"*/*.jpg\")))\n",
        "\n",
        "list_ds = tf.data.Dataset.list_files(str(IMAGE_PATH/\"*/*.jpg\"))\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "labeled_ds = labeled_ds.shuffle(IMAGE_COUNT)\n",
        "\n",
        "train_size = int(0.6 * IMAGE_COUNT)\n",
        "valid_size = int(0.2 * IMAGE_COUNT)\n",
        "\n",
        "train_ds = labeled_ds.take(train_size)\n",
        "test_ds = labeled_ds.skip(train_size)\n",
        "valid_ds = test_ds.take(valid_size)\n",
        "test_ds = test_ds.skip(valid_size)\n",
        "\n",
        "train_ds = prepare_for_training(train_ds, shuffle_buffer_size=train_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}